{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a626e20197e21c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Using ZhipuAI GLM-4 though OpenAI SDK and Ecosystem\n",
    "\n",
    "**This tutorial is available in English and is attached below the Chinese explanation**\n",
    "\n",
    "本节代码将带领大家使用`OpenAI`的 SDK 和支持的框架调用 `GLM-4` 模型\n",
    "你将在本章节学习到如何使用 Langchain 的 `ChatOpenAI` 类来调用 ZhipuAI 的 GLM-4 模型，以及如何使用 `OpenAI SDK` 类来实现调用GLM-4模型。\n",
    "\n",
    "This notebook section will walk you through calling `GLM-4` models using the `OpenAI` SDK and supporting frameworks.\n",
    "In this section you will learn how to call ZhipuAI's GLM-4 models using Langchain's `ChatOpenAI` class, and how to use the `OpenAI SDK` class to implement calling GLM-4 models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e7836f5b885b6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Use OpenAI SDK\n",
    "\n",
    "首先，需要导入 `OpenAI` SDK，同时，修改 `OpenAI` 的 `base_url` 为 `https://open.bigmodel.cn/api/paas/v4/` \n",
    "以便调用 `GLM-4` 模型。完成更换后，仅需填写 Key 就能使用 `GLM-4` 模型。\n",
    "\n",
    "First, you need to import the `OpenAI` SDK and change the `OpenAI` `base_url` to `https://open.bigmodel.cn/api/paas/v4/` in order to call the `GLM-4` model. \n",
    "in order to call the `GLM-4` model. Once you have done this, you can use the `GLM-4` model by simply filling in the Key.\n",
    "``python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T03:38:45.899372Z",
     "start_time": "2024-04-23T03:38:45.633362Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"ZHIPUAI_API_KEY\"),\n",
    "    base_url=\"https://open.bigmodel.cn/api/paas/v4/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3e2b3eddb8819",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Simple use of OpenAI SDK to call GLM-4\n",
    "\n",
    "我将带领大家完成最基础的对话调用。\n",
    "请注意：\n",
    "- `temperature` 参数的区间为 (0,1)。\n",
    "- `do_sample = False (temperature = 0)` 在 OpenAI 调用中并不适用。\n",
    "-  异步操作不适用。\n",
    "\n",
    "其他例子(例如工具调用)，也均可以按照OpenAI的方式进行调用。\n",
    "\n",
    "\n",
    "I will lead everyone to complete the most basic dialogue calls.\n",
    "Please note:\n",
    "\n",
    "- The interval of the `temperature` parameter is (0,1).\n",
    "- `do_sample=False (temperature=0)` is not applicable in OpenAI calls.\n",
    "- Asynchronous operations are not applicable.\n",
    "\n",
    "Other examples (such as tool calls) can also be called in the same way as OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b558c16e324725",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T03:38:47.447042Z",
     "start_time": "2024-04-23T03:38:45.900671Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='8602219229372288758', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure, here's a light-hearted joke for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\", role='assistant', function_call=None, tool_calls=None))], created=1714098003, model='glm-4', object=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=26, prompt_tokens=9, total_tokens=35), request_id='8602219229372288758')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"tell me a joke\"\n",
    "        }\n",
    "    ],\n",
    "    top_p=0.7,\n",
    "    temperature=0.9,\n",
    "    stream=False,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96faeabc4754841",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba894923a1806fc2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Using Langchain's ChatOpenAI class\n",
    "\n",
    "Langchain 的 `ChatOpenAI` 类是对 `OpenAI` SDK 的封装，可以更方便调用。这里展示了如何使用 `ChatOpenAI` 类来调用 `GLM-4` 模型。\n",
    "\n",
    "Langchain's `ChatOpenAI` class is a wrapper around the `OpenAI` SDK, making it easier to call. Here we show how to use the `ChatOpenAI` class to call the `GLM-4` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95ca98869dcb246a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T03:42:00.468839Z",
     "start_time": "2024-04-23T03:41:59.394044Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: tell me a joke\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'tell me a joke',\n",
       " 'chat_history': [HumanMessage(content='tell me a joke'),\n",
       "  AIMessage(content=\"Sure! Here's a light-hearted joke for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything! 😄\")],\n",
       " 'text': \"Sure! Here's a light-hearted joke for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything! 😄\"}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.01,\n",
    "    model=\"glm-4\",\n",
    "    openai_api_key=os.getenv(\"ZHIPUAI_API_KEY\"),\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "conversation.invoke({\"question\": \"tell me a joke\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6c560191f25ec5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Calling GLM-4 with LangChain AgentExecutor\n",
    "同时，GLM-4也能很好的接入到 LangChain 的 AgentExecutor 中，这里展示了如何使用 `AgentExecutor` 来调用 `GLM-4` 模型。\n",
    "\n",
    "GLM-4 also plugs nicely into LangChain's AgentExecutor, which shows how to call a `GLM-4` model using `AgentExecutor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db6cc6154c30560",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T03:39:14.321435Z",
     "start_time": "2024-04-23T03:38:47.990959Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThese seem to be specific functions or agents related to a particular software framework or system. I am not familiar with these specific functions, but I can try to search for information about them. \n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: differences between create_tool_calling_agent and create_react_agent,create_json_chat_agent\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://api.python.langchain.com/en/latest/agents/langchain.agents.react.agent.create_react_agent.html', 'content': 'A Runnable sequence representing an agent. It takes as input all the same input variables as the prompt passed in does. It returns as output either an AgentAction or AgentFinish. Examples: from langchain import hub from langchain_community.llms import OpenAI from langchain.agents import AgentExecutor, create_react_agent prompt = hub.pull ...'}, {'url': 'https://medium.com/@emil.azadian/using-agents-as-tools-in-langchain-1758878792b2', 'content': 'The docs suggest using the agent.run method as function parameter to the StructuredTool.from_function method, which I tried first\\nSince StructuredTool uses pydantic to translate data types and docstrings of the func parameter into JSON (to provide that to the using agent as extra info), we are running into an error here:\\nI guess this could be fixed by setting infer_schema to False and explicitly passing the function schema excluding the callback field to the from_function method. Sign up\\nSign in\\nSign up\\nSign in\\nUsing Agents as Tools in LangChain\\nEmil Azadian\\nFollow\\n--\\nListen\\nShare\\nIf you’ve just started looking into LangChain and wonder how you could use agents as tools for other agents, you’ve come to the right place. Credits to Hamid Khodabakhshandeh, with whom I explored this topic at a recent hackathon :)\\n--\\n--\\nWritten by Emil Azadian\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams Attempt 2 — Creating a tool from a function running the agent\\nHere, we simply define a function that runs the agent for us, and then build a tool from that function:\\nwe can then go on and define an agent that uses this agent as a tool. Here is a small excerpt of the agent’s verbose output:\\nYou can see how the main agent uses the agent tool, and how that triggers another AgentExecutor chain, which is exactly what we wanted.\\n'}]\u001b[0m\u001b[32;1m\u001b[1;3mBased on my search results, it seems that create_tool_calling_agent, create_react_agent, and create_json_chat_agent are functions related to LangChain, which is a framework for building agents and AI models. The specific differences between these functions are not clearly mentioned in the search results. However, I can provide some information based on the search results:\n",
      "\n",
      "- create_react_agent: This function is used to create a Runnable sequence representing an agent in LangChain. It takes input variables and returns either an AgentAction or AgentFinish.\n",
      "\n",
      "- create_tool_calling_agent: There is no specific information available about this function in the search results. It might be a custom function or a variation of the agent-creation functions in LangChain.\n",
      "\n",
      "- create_json_chat_agent: There is no specific information available about this function in the search results. It might be a function related to creating a JSON-based chat agent or a tool in LangChain.\n",
      "\n",
      "To get a more accurate understanding of the differences between these functions, it would be helpful to refer to the official LangChain documentation or seek further information from reliable sources.\n",
      "\n",
      "Final Answer: The differences between create_tool_calling_agent, create_react_agent, and create_json_chat_agent are not clearly mentioned in the available search results. It is recommended to refer to the LangChain documentation or explore reliable sources for more accurate information.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'create_tool_calling_agent 和 create_react_agent,create_json_chat_agent 的区别?',\n",
       " 'output': 'The differences between create_tool_calling_agent, create_react_agent, and create_json_chat_agent are not clearly mentioned in the available search results. It is recommended to refer to the LangChain documentation or explore reliable sources for more accurate information.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent,create_tool_calling_agent,initialize_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# os.environ[\"TAVILY_API_KEY\"] = \"tvly-xxx\"\n",
    "tools = [TavilySearchResults(max_results=2)]\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "# Choose the LLM to use\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "# agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"create_tool_calling_agent 和 create_react_agent,create_json_chat_agent 的区别?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
