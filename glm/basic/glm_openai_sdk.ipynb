{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a626e20197e21c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Using ZhipuAI GLM-4 though OpenAI SDK and Ecosystem\n",
    "\n",
    "**This tutorial is available in English and is attached below the Chinese explanation**\n",
    "\n",
    "æœ¬èŠ‚ä»£ç å°†å¸¦é¢†å¤§å®¶ä½¿ç”¨`OpenAI`çš„ SDK å’Œæ”¯æŒçš„æ¡†æž¶è°ƒç”¨ `GLM-4` æ¨¡åž‹\n",
    "ä½ å°†åœ¨æœ¬ç« èŠ‚å­¦ä¹ åˆ°å¦‚ä½•ä½¿ç”¨ Langchain çš„ `ChatOpenAI` ç±»æ¥è°ƒç”¨ ZhipuAI çš„ GLM-4 æ¨¡åž‹ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ `OpenAI SDK` ç±»æ¥å®žçŽ°è°ƒç”¨GLM-4æ¨¡åž‹ã€‚\n",
    "\n",
    "This notebook section will walk you through calling `GLM-4` models using the `OpenAI` SDK and supporting frameworks.\n",
    "In this section you will learn how to call ZhipuAI's GLM-4 models using Langchain's `ChatOpenAI` class, and how to use the `OpenAI SDK` class to implement calling GLM-4 models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e7836f5b885b6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Use OpenAI SDK\n",
    "\n",
    "é¦–å…ˆï¼Œéœ€è¦å¯¼å…¥ `OpenAI` SDKï¼ŒåŒæ—¶ï¼Œä¿®æ”¹ `OpenAI` çš„ `base_url` ä¸º `https://open.bigmodel.cn/api/paas/v4/` \n",
    "ä»¥ä¾¿è°ƒç”¨ `GLM-4` æ¨¡åž‹ã€‚å®Œæˆæ›´æ¢åŽï¼Œä»…éœ€å¡«å†™ Key å°±èƒ½ä½¿ç”¨ `GLM-4` æ¨¡åž‹ã€‚\n",
    "\n",
    "First, you need to import the `OpenAI` SDK and change the `OpenAI` `base_url` to `https://open.bigmodel.cn/api/paas/v4/` in order to call the `GLM-4` model. \n",
    "in order to call the `GLM-4` model. Once you have done this, you can use the `GLM-4` model by simply filling in the Key.\n",
    "``python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T03:38:45.899372Z",
     "start_time": "2024-04-23T03:38:45.633362Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"ZHIPUAI_API_KEY\"),\n",
    "    base_url=\"https://open.bigmodel.cn/api/paas/v4/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3e2b3eddb8819",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Simple use of OpenAI SDK to call GLM-4\n",
    "\n",
    "æˆ‘å°†å¸¦é¢†å¤§å®¶å®Œæˆæœ€åŸºç¡€çš„å¯¹è¯è°ƒç”¨ã€‚\n",
    "è¯·æ³¨æ„ï¼š\n",
    "- `temperature` å‚æ•°çš„åŒºé—´ä¸º (0,1)ã€‚\n",
    "- `do_sample = False (temperature = 0)` åœ¨ OpenAI è°ƒç”¨ä¸­å¹¶ä¸é€‚ç”¨ã€‚\n",
    "-  å¼‚æ­¥æ“ä½œä¸é€‚ç”¨ã€‚\n",
    "\n",
    "å…¶ä»–ä¾‹å­(ä¾‹å¦‚å·¥å…·è°ƒç”¨)ï¼Œä¹Ÿå‡å¯ä»¥æŒ‰ç…§OpenAIçš„æ–¹å¼è¿›è¡Œè°ƒç”¨ã€‚\n",
    "\n",
    "\n",
    "I will lead everyone to complete the most basic dialogue calls.\n",
    "Please note:\n",
    "\n",
    "- The interval of the `temperature` parameter is (0,1).\n",
    "- `do_sample=False (temperature=0)` is not applicable in OpenAI calls.\n",
    "- Asynchronous operations are not applicable.\n",
    "\n",
    "Other examples (such as tool calls) can also be called in the same way as OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b558c16e324725",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T03:38:47.447042Z",
     "start_time": "2024-04-23T03:38:45.900671Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='8602219229372288758', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure, here's a light-hearted joke for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\", role='assistant', function_call=None, tool_calls=None))], created=1714098003, model='glm-4', object=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=26, prompt_tokens=9, total_tokens=35), request_id='8602219229372288758')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"tell me a joke\"\n",
    "        }\n",
    "    ],\n",
    "    top_p=0.7,\n",
    "    temperature=0.9,\n",
    "    stream=False,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96faeabc4754841",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba894923a1806fc2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Using Langchain's ChatOpenAI class\n",
    "\n",
    "Langchain çš„ `ChatOpenAI` ç±»æ˜¯å¯¹ `OpenAI` SDK çš„å°è£…ï¼Œå¯ä»¥æ›´æ–¹ä¾¿è°ƒç”¨ã€‚è¿™é‡Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `ChatOpenAI` ç±»æ¥è°ƒç”¨ `GLM-4` æ¨¡åž‹ã€‚\n",
    "\n",
    "Langchain's `ChatOpenAI` class is a wrapper around the `OpenAI` SDK, making it easier to call. Here we show how to use the `ChatOpenAI` class to call the `GLM-4` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95ca98869dcb246a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T03:42:00.468839Z",
     "start_time": "2024-04-23T03:41:59.394044Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: tell me a joke\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'tell me a joke',\n",
       " 'chat_history': [HumanMessage(content='tell me a joke'),\n",
       "  AIMessage(content=\"Sure! Here's a light-hearted joke for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything! ðŸ˜„\")],\n",
       " 'text': \"Sure! Here's a light-hearted joke for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything! ðŸ˜„\"}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.01,\n",
    "    model=\"glm-4\",\n",
    "    openai_api_key=os.getenv(\"ZHIPUAI_API_KEY\"),\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "conversation.invoke({\"question\": \"tell me a joke\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6c560191f25ec5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Calling GLM-4 with LangChain AgentExecutor\n",
    "åŒæ—¶ï¼ŒGLM-4ä¹Ÿèƒ½å¾ˆå¥½çš„æŽ¥å…¥åˆ° LangChain çš„ AgentExecutor ä¸­ï¼Œè¿™é‡Œå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `AgentExecutor` æ¥è°ƒç”¨ `GLM-4` æ¨¡åž‹ã€‚\n",
    "\n",
    "GLM-4 also plugs nicely into LangChain's AgentExecutor, which shows how to call a `GLM-4` model using `AgentExecutor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db6cc6154c30560",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T03:39:14.321435Z",
     "start_time": "2024-04-23T03:38:47.990959Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThese seem to be specific functions or agents related to a particular software framework or system. I am not familiar with these specific functions, but I can try to search for information about them. \n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: differences between create_tool_calling_agent and create_react_agent,create_json_chat_agent\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://api.python.langchain.com/en/latest/agents/langchain.agents.react.agent.create_react_agent.html', 'content': 'A Runnable sequence representing an agent. It takes as input all the same input variables as the prompt passed in does. It returns as output either an AgentAction or AgentFinish. Examples: from langchain import hub from langchain_community.llms import OpenAI from langchain.agents import AgentExecutor, create_react_agent prompt = hub.pull ...'}, {'url': 'https://medium.com/@emil.azadian/using-agents-as-tools-in-langchain-1758878792b2', 'content': 'The docs suggest using the agent.run method as function parameter to the StructuredTool.from_function method, which I tried first\\nSince StructuredTool uses pydantic to translate data types and docstrings of the func parameter into JSON (to provide that to the using agent as extra info), we are running into an error here:\\nI guess this could be fixed by setting infer_schema to False and explicitly passing the function schema excluding the callback field to the from_function method. Sign up\\nSign in\\nSign up\\nSign in\\nUsing Agents as Tools in LangChain\\nEmil Azadian\\nFollow\\n--\\nListen\\nShare\\nIf youâ€™ve just started looking into LangChain and wonder how you could use agents as tools for other agents, youâ€™ve come to the right place. Credits to Hamid Khodabakhshandeh, with whom I explored this topic at a recent hackathon :)\\n--\\n--\\nWritten by Emil Azadian\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams Attempt 2 â€” Creating a tool from a function running the agent\\nHere, we simply define a function that runs the agent for us, and then build a tool from that function:\\nwe can then go on and define an agent that uses this agent as a tool. Here is a small excerpt of the agentâ€™s verbose output:\\nYou can see how the main agent uses the agent tool, and how that triggers another AgentExecutor chain, which is exactly what we wanted.\\n'}]\u001b[0m\u001b[32;1m\u001b[1;3mBased on my search results, it seems that create_tool_calling_agent, create_react_agent, and create_json_chat_agent are functions related to LangChain, which is a framework for building agents and AI models. The specific differences between these functions are not clearly mentioned in the search results. However, I can provide some information based on the search results:\n",
      "\n",
      "- create_react_agent: This function is used to create a Runnable sequence representing an agent in LangChain. It takes input variables and returns either an AgentAction or AgentFinish.\n",
      "\n",
      "- create_tool_calling_agent: There is no specific information available about this function in the search results. It might be a custom function or a variation of the agent-creation functions in LangChain.\n",
      "\n",
      "- create_json_chat_agent: There is no specific information available about this function in the search results. It might be a function related to creating a JSON-based chat agent or a tool in LangChain.\n",
      "\n",
      "To get a more accurate understanding of the differences between these functions, it would be helpful to refer to the official LangChain documentation or seek further information from reliable sources.\n",
      "\n",
      "Final Answer: The differences between create_tool_calling_agent, create_react_agent, and create_json_chat_agent are not clearly mentioned in the available search results. It is recommended to refer to the LangChain documentation or explore reliable sources for more accurate information.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'create_tool_calling_agent å’Œ create_react_agent,create_json_chat_agent çš„åŒºåˆ«?',\n",
       " 'output': 'The differences between create_tool_calling_agent, create_react_agent, and create_json_chat_agent are not clearly mentioned in the available search results. It is recommended to refer to the LangChain documentation or explore reliable sources for more accurate information.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent,create_tool_calling_agent,initialize_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# os.environ[\"TAVILY_API_KEY\"] = \"tvly-xxx\"\n",
    "tools = [TavilySearchResults(max_results=2)]\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "# Choose the LLM to use\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "# agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"create_tool_calling_agent å’Œ create_react_agent,create_json_chat_agent çš„åŒºåˆ«?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
