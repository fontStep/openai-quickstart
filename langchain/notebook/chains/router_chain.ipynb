{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain_openai import OpenAI,ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(model=MODEL_NAME, openai_proxy=os.getenv(\"OPENAI_BASE_URL\"),max_tokens=1000,api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"你是一位非常聪明的物理教授。\n",
    "你擅长以简洁易懂的方式回答关于物理的问题。\n",
    "当你不知道某个问题的答案时，你会坦诚承认。\n",
    "\n",
    "这是一个问题：\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"你是一位很棒的数学家。你擅长回答数学问题。\n",
    "之所以如此出色，是因为你能够将难题分解成各个组成部分，\n",
    "先回答这些组成部分，然后再将它们整合起来回答更广泛的问题。\n",
    "\n",
    "这是一个问题：\n",
    "{input}\"\"\"\n",
    "\n",
    "biology_template = \"\"\"你是一位非常聪明的生物学家。\n",
    "你擅长以简洁易懂的方式回答关于生物学的问题。\n",
    "当你不知道某个问题的答案时，你会坦诚承认。\n",
    "\n",
    "这是一个问题：\n",
    "{input}\"\"\"\n",
    "\n",
    "computer_template = \"\"\"你是一位非常聪明的计算机科学家。\n",
    "你擅长以简洁易懂的方式回答关于计算机的问题。\n",
    "当你不知道某个问题的答案时，你会坦诚承认。\n",
    "\n",
    "这是一个问题：\n",
    "{input}\"\"\"\n",
    "\n",
    "chinese_template = \"\"\"你是一位非常聪明的语文老师。\n",
    "你擅长以简洁易懂的方式回答关于语文的问题。\n",
    "当你不知道某个问题的答案时，你会坦诚承认。\n",
    "\n",
    "这是一个问题：\n",
    "{input}\"\"\"\n",
    "\n",
    "flower_care_template = \"\"\"\n",
    "你是一个经验丰富的园丁，擅长解答关于养花育花的问题。\n",
    "下面是需要你来回答的问题:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "flower_deco_template = \"\"\"\n",
    "你是一位网红插花大师，擅长解答关于鲜花装饰的问题。\n",
    "下面是需要你来回答的问题:\n",
    "{input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"物理\",\n",
    "        \"description\": \"适用于回答物理问题\",\n",
    "        \"prompt_template\": physics_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"数学\",\n",
    "        \"description\": \"适用于回答数学问题\",\n",
    "        \"prompt_template\": math_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\":\"生物\",\n",
    "        \"description\": \"适用于回答生物问题\",\n",
    "         \"prompt_template\": biology_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"语文\",\n",
    "        \"description\": \"适用于回答语文问题\",\n",
    "        \"prompt_template\": chinese_template,\n",
    "    },{\n",
    "        \"name\":\"计算机\",\n",
    "        \"description\": \"适用于回答计算机问题\",\n",
    "         \"prompt_template\": computer_template,\n",
    "    },\n",
    "     {\n",
    "        \"name\": \"鲜花护理\",\n",
    "        \"description\": \"适合回答关于鲜花护理的问题\",\n",
    "        \"prompt_template\": flower_care_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"鲜花装饰\",\n",
    "        \"description\": \"适合回答关于鲜花装饰的问题\",\n",
    "        \"prompt_template\": flower_deco_template,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个空的目标链字典，用于存放根据prompt_infos生成的LLMChain。\n",
    "destination_chains = {}\n",
    "\n",
    "# 遍历prompt_infos列表，为每个信息创建一个LLMChain。\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]  # 提取名称\n",
    "    prompt_template = p_info[\"prompt_template\"]  # 提取模板\n",
    "    # 创建PromptTemplate对象\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    # 使用上述模板和llm对象创建LLMChain对象\n",
    "    chain = LLMChain(llm=llm, prompt=prompt,verbose=True)\n",
    "    # 将新创建的chain对象添加到destination_chains字典中\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "# 创建一个默认的ConversationChain\n",
    "default_chain = ConversationChain(llm=llm, output_key=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'物理': LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['input'], template='你是一位非常聪明的物理教授。\\n你擅长以简洁易懂的方式回答关于物理的问题。\\n当你不知道某个问题的答案时，你会坦诚承认。\\n\\n这是一个问题：\\n{input}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001958FD42CE0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001958FD44D90>, openai_api_key=SecretStr('**********'), openai_proxy='https://api.openai-proxy.com/v1', max_tokens=1000)),\n",
       " '数学': LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['input'], template='你是一位很棒的数学家。你擅长回答数学问题。\\n之所以如此出色，是因为你能够将难题分解成各个组成部分，\\n先回答这些组成部分，然后再将它们整合起来回答更广泛的问题。\\n\\n这是一个问题：\\n{input}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001958FD42CE0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001958FD44D90>, openai_api_key=SecretStr('**********'), openai_proxy='https://api.openai-proxy.com/v1', max_tokens=1000)),\n",
       " '生物': LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['input'], template='你是一位非常聪明的生物学家。\\n你擅长以简洁易懂的方式回答关于生物学的问题。\\n当你不知道某个问题的答案时，你会坦诚承认。\\n\\n这是一个问题：\\n{input}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001958FD42CE0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001958FD44D90>, openai_api_key=SecretStr('**********'), openai_proxy='https://api.openai-proxy.com/v1', max_tokens=1000)),\n",
       " '语文': LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['input'], template='你是一位非常聪明的语文老师。\\n你擅长以简洁易懂的方式回答关于语文的问题。\\n当你不知道某个问题的答案时，你会坦诚承认。\\n\\n这是一个问题：\\n{input}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001958FD42CE0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001958FD44D90>, openai_api_key=SecretStr('**********'), openai_proxy='https://api.openai-proxy.com/v1', max_tokens=1000)),\n",
       " '计算机': LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['input'], template='你是一位非常聪明的计算机科学家。\\n你擅长以简洁易懂的方式回答关于计算机的问题。\\n当你不知道某个问题的答案时，你会坦诚承认。\\n\\n这是一个问题：\\n{input}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001958FD42CE0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001958FD44D90>, openai_api_key=SecretStr('**********'), openai_proxy='https://api.openai-proxy.com/v1', max_tokens=1000)),\n",
       " '鲜花护理': LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['input'], template='\\n你是一个经验丰富的园丁，擅长解答关于养花育花的问题。\\n下面是需要你来回答的问题:\\n{input}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001958FD42CE0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001958FD44D90>, openai_api_key=SecretStr('**********'), openai_proxy='https://api.openai-proxy.com/v1', max_tokens=1000)),\n",
       " '鲜花装饰': LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['input'], template='\\n你是一位网红插花大师，擅长解答关于鲜花装饰的问题。\\n下面是需要你来回答的问题:\\n{input}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001958FD42CE0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001958FD44D90>, openai_api_key=SecretStr('**********'), openai_proxy='https://api.openai-proxy.com/v1', max_tokens=1000))}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(destination_chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': '物理',\n",
       "  'description': '适用于回答物理问题',\n",
       "  'prompt_template': '你是一位非常聪明的物理教授。\\n你擅长以简洁易懂的方式回答关于物理的问题。\\n当你不知道某个问题的答案时，你会坦诚承认。\\n\\n这是一个问题：\\n{input}'},\n",
       " {'name': '数学',\n",
       "  'description': '适用于回答数学问题',\n",
       "  'prompt_template': '你是一位很棒的数学家。你擅长回答数学问题。\\n之所以如此出色，是因为你能够将难题分解成各个组成部分，\\n先回答这些组成部分，然后再将它们整合起来回答更广泛的问题。\\n\\n这是一个问题：\\n{input}'},\n",
       " {'name': '生物',\n",
       "  'description': '适用于回答生物问题',\n",
       "  'prompt_template': '你是一位非常聪明的生物学家。\\n你擅长以简洁易懂的方式回答关于生物学的问题。\\n当你不知道某个问题的答案时，你会坦诚承认。\\n\\n这是一个问题：\\n{input}'},\n",
       " {'name': '语文',\n",
       "  'description': '适用于回答语文问题',\n",
       "  'prompt_template': '你是一位非常聪明的语文老师。\\n你擅长以简洁易懂的方式回答关于语文的问题。\\n当你不知道某个问题的答案时，你会坦诚承认。\\n\\n这是一个问题：\\n{input}'},\n",
       " {'name': '计算机',\n",
       "  'description': '适用于回答计算机问题',\n",
       "  'prompt_template': '你是一位非常聪明的计算机科学家。\\n你擅长以简洁易懂的方式回答关于计算机的问题。\\n当你不知道某个问题的答案时，你会坦诚承认。\\n\\n这是一个问题：\\n{input}'},\n",
       " {'name': '鲜花护理',\n",
       "  'description': '适合回答关于鲜花护理的问题',\n",
       "  'prompt_template': '\\n你是一个经验丰富的园丁，擅长解答关于养花育花的问题。\\n下面是需要你来回答的问题:\\n{input}\\n'},\n",
       " {'name': '鲜花装饰',\n",
       "  'description': '适合回答关于鲜花装饰的问题',\n",
       "  'prompt_template': '\\n你是一位网红插花大师，擅长解答关于鲜花装饰的问题。\\n下面是需要你来回答的问题:\\n{input}\\n'}]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.chains.conversation.base.ConversationChain"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(default_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从prompt_infos中提取目标信息并将其转化为字符串列表\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "# 使用join方法将列表转化为字符串，每个元素之间用换行符分隔\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "# 根据MULTI_PROMPT_ROUTER_TEMPLATE格式化字符串和destinations_str创建路由模板\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "# 创建路由的PromptTemplate\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    "    \n",
    ")\n",
    "# 使用上述路由模板和llm对象创建LLMRouterChain对象\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['物理: 适用于回答物理问题', '数学: 适用于回答数学问题', '生物: 适用于回答生物问题', '语文: 适用于回答语文问题', '计算机: 适用于回答计算机问题', '鲜花护理: 适合回答关于鲜花护理的问题', '鲜花装饰: 适合回答关于鲜花装饰的问题']\n"
     ]
    }
   ],
   "source": [
    "print(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "物理: 适用于回答物理问题\n",
      "数学: 适用于回答数学问题\n",
      "生物: 适用于回答生物问题\n",
      "语文: 适用于回答语文问题\n",
      "计算机: 适用于回答计算机问题\n",
      "鲜花护理: 适合回答关于鲜花护理的问题\n",
      "鲜花装饰: 适合回答关于鲜花装饰的问题\n"
     ]
    }
   ],
   "source": [
    "print(destinations_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "物理: 适用于回答物理问题\n",
      "数学: 适用于回答数学问题\n",
      "生物: 适用于回答生物问题\n",
      "语文: 适用于回答语文问题\n",
      "计算机: 适用于回答计算机问题\n",
      "鲜花护理: 适合回答关于鲜花护理的问题\n",
      "鲜花装饰: 适合回答关于鲜花装饰的问题\n",
      "\n",
      "<< INPUT >>\n",
      "{input}\n",
      "\n",
      "<< OUTPUT (must include ```json at the start of the response) >>\n",
      "<< OUTPUT (must end with ```) >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(router_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "{destinations}\n",
      "\n",
      "<< INPUT >>\n",
      "{{input}}\n",
      "\n",
      "<< OUTPUT (must include ```json at the start of the response) >>\n",
      "<< OUTPUT (must end with ```) >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(MULTI_PROMPT_ROUTER_TEMPLATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建MultiPromptChain对象，其中包含了路由链，目标链和默认链。\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMRouterChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "物理: {'input': 'What is blackbody radiation?'}\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m你是一位非常聪明的物理教授。\n",
      "你擅长以简洁易懂的方式回答关于物理的问题。\n",
      "当你不知道某个问题的答案时，你会坦诚承认。\n",
      "\n",
      "这是一个问题：\n",
      "What is blackbody radiation?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'What is blackbody radiation?', 'text': \"Blackbody radiation refers to the electromagnetic radiation emitted by a perfect absorber and emitter of radiation, known as a blackbody. A blackbody absorbs all radiation incident on it and emits radiation at all wavelengths. The spectrum of blackbody radiation is continuous and follows a specific distribution known as Planck's law. This phenomena is important in understanding the behavior of objects at different temperatures and in various applications in physics and engineering.\"}\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"黑体辐射是什么？?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMRouterChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "数学: {'input': '找出大于40的第一个质数，使得这个质数加一能被3整除。'}\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m你是一位很棒的数学家。你擅长回答数学问题。\n",
      "之所以如此出色，是因为你能够将难题分解成各个组成部分，\n",
      "先回答这些组成部分，然后再将它们整合起来回答更广泛的问题。\n",
      "\n",
      "这是一个问题：\n",
      "找出大于40的第一个质数，使得这个质数加一能被3整除。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '找出大于40的第一个质数，使得这个质数加一能被3整除。', 'text': '首先，我们要找出大于40的质数。质数是只能被1和自身整除的数，所以我们从41开始逐个判断是否为质数。\\n\\n41是质数，然后我们判断41加一是否能被3整除，即42是否能被3整除。42除以3余数为0，所以42可以被3整除。\\n\\n因此，大于40的第一个质数是41，且41加一等于42可以被3整除。\\n\\n所以，答案是41。'}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    chain.invoke(\n",
    "        \"大于40的第一个质数是多少，使得这个质数加一能被3整除？\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMRouterChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "语文: {'input': '红楼梦的作者是谁？'}\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m你是一位非常聪明的语文老师。\n",
      "你擅长以简洁易懂的方式回答关于语文的问题。\n",
      "当你不知道某个问题的答案时，你会坦诚承认。\n",
      "\n",
      "这是一个问题：\n",
      "红楼梦的作者是谁？\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '红楼梦的作者是谁？', 'text': '红楼梦的作者是曹雪芹。'}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    chain.invoke(\n",
    "        \"红楼梦的作者是谁？\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMRouterChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "鲜花护理: {'input': '如何为玫瑰浇水？'}\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "你是一个经验丰富的园丁，擅长解答关于养花育花的问题。\n",
      "下面是需要你来回答的问题:\n",
      "如何为玫瑰浇水？\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '如何为玫瑰浇水？', 'text': '玫瑰是比较耐旱的植物，但也需要适量的水分来保持生长和健康。一般来说，玫瑰的浇水要注意以下几点：\\n\\n1. 浇水要适量：不要让玫瑰根部长时间浸泡在水中，也不要让土壤过于干燥。一般来说，每周浇水一到两次，保持土壤微湿但不过湿。\\n\\n2. 避免叶子浇水：尽量避免直接将水浇在玫瑰的叶子上，特别是在阳光强烈的时候，这样容易导致叶子烧伤。\\n\\n3. 早晨浇水：最好在早晨或傍晚时分浇水，避免在阳光直射的时候浇水，以免水滴在叶子上引起光合作用不良。\\n\\n4. 使用深根浇水法：尽量让水渗透到玫瑰的根部深处，这样能够促进根系生长，使植物更加健壮。\\n\\n总的来说，保持适度的湿润是玫瑰生长的关键，避免过湿或过干都是很重要的。希望以上建议对您有所帮助！如果有其他问题，请随时向我提问。'}\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke('如何为玫瑰浇水？'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
