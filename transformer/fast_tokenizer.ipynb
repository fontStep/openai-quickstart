{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 快速分词器\n",
    "前面我们已经介绍过如何使用分词器将文本编码为 token IDs，以及反过来将 token IDs 解码回文本。\n",
    "\n",
    "实际上，Hugging Face 共提供了两种分分词器：\n",
    "\n",
    "慢速分词器：Transformers 库自带，使用 Python 编写；\n",
    "快速分词器：Tokenizers 库提供，使用 Rust 编写。\n",
    "特别地，快速分词器除了能进行编码和解码之外，还能够追踪原文到 token 之间的映射，这对于处理序列标注、自动问答等任务非常重要。\n",
    "\n",
    "快速分词器只有在并行处理大量文本时才能发挥出速度优势，在处理单个句子时甚至可能慢于慢速分词器。\n",
    "\n",
    "我们一直推荐使用的 AutoTokenizer 类除了能根据 checkpoint 自动加载对应分词器以外，默认就会选择快速分词器，因此在大部分情况下都应该使用 AutoTokenizer 类来加载分词器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 再看分词效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8667, 1362, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "tokenizer.is_fast: True\n",
      "encoding.is_fast: True\n",
      "encoding.input_ids: ['[CLS]', 'Hello', 'world', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "example = \"Hello world!\"\n",
    "encoding = tokenizer(example)\n",
    "print(encoding)\n",
    "print(type(encoding))\n",
    "print('tokenizer.is_fast:', tokenizer.is_fast)\n",
    "print('encoding.is_fast:', encoding.is_fast)\n",
    "print('encoding.input_ids:', encoding.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in', 'Brooklyn', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "encoding = tokenizer(example)\n",
    "print(encoding.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 追踪映射\n",
    "在上面的例子中，索引为 5 的 token 是“##yl”，它是词语“Sylvain”的一个部分，因此在映射回原文时不应该被单独看待。我们可以通过 word_ids() 函数来获取每一个 token 对应的词语索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]\n",
      "[101, 1422, 1271, 1110, 156, 7777, 2497, 1394, 1105, 146, 1250, 1120, 20164, 10932, 10289, 1107, 6010, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "print(encoding.word_ids())\n",
    "print(encoding.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5th token is: ##yl\n",
      "start and end of the token is: 12 14 (inclusive)\n",
      "example is: My name is Sylvain and I work at Hugging Face in Brooklyn.\n",
      "corresponding text span is: yl\n",
      "the word index is: 3\n",
      "start and end of the token is: 11 18 (inclusive)\n",
      "corresponding word span is: Sylvain\n"
     ]
    }
   ],
   "source": [
    "# 词语/token \n",
    "#  文本：通过 word_to_chars()、token_to_chars() 函数来实现，返回词语/token 在原文中的起始和结束偏移量。\n",
    "token_index = 5\n",
    "print('the 5th token is:', encoding.tokens()[token_index])\n",
    "start, end = encoding.token_to_chars(token_index)\n",
    "print('start and end of the token is:', start, end, '(inclusive)')\n",
    "print('example is:', example)\n",
    "print('corresponding text span is:', example[start:end])\n",
    "word_index = encoding.word_ids()[token_index] # 3\n",
    "print('the word index is:', word_index)\n",
    "start, end = encoding.word_to_chars(word_index)\n",
    "print('start and end of the token is:', start, end, '(inclusive)')\n",
    "print('corresponding word span is:', example[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5th token is: ##yl\n",
      "corresponding word index is: 3\n",
      "the word is: Sylvain\n",
      "corresponding tokens are: ['S', '##yl', '##va', '##in']\n"
     ]
    }
   ],
   "source": [
    "# 词语 \n",
    "#  token：前面的例子中我们使用 word_ids() 获取了整个 token 序列对应的词语索引。\n",
    "#  实际上，词语和 token 之间可以直接通过索引直接映射，分别通过 token_to_word() 和 word_to_tokens() 来实现\n",
    "token_index = 5\n",
    "print('the 5th token is:', encoding.tokens()[token_index])\n",
    "corresp_word_index = encoding.token_to_word(token_index)\n",
    "print('corresponding word index is:', corresp_word_index)\n",
    "start, end = encoding.word_to_chars(corresp_word_index)\n",
    "print('the word is:', example[start:end])\n",
    "start, end = encoding.word_to_tokens(corresp_word_index)\n",
    "print('corresponding tokens are:', encoding.tokens()[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters of \"My name is Sylvain\" ars: ['M', 'y', ' ', 'n', 'a', 'm', 'e', ' ', 'i', 's', ' ', 'S', 'y', 'l', 'v', 'a', 'i', 'n']\n",
      "corresponding word index: \n",
      "\"M\": 0 \"y\": 0 \" \": None \"n\": 1 \"a\": 1 \"m\": 1 \"e\": 1 \" \": None \"i\": 2 \"s\": 2 \" \": None \"S\": 3 \"y\": 3 \"l\": 3 \"v\": 3 \"a\": 3 \"i\": 3 \"n\": 3 \n",
      "word_ids: [None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]\n",
      "\n",
      "corresponding token index: \n",
      "\"M\": 1 \"y\": 1 \" \": None \"n\": 2 \"a\": 2 \"m\": 2 \"e\": 2 \" \": None \"i\": 3 \"s\": 3 \" \": None \"S\": 4 \"y\": 5 \"l\": 5 \"v\": 6 \"a\": 6 \"i\": 7 \"n\": 7 "
     ]
    }
   ],
   "source": [
    "# 文本 \n",
    "#  词语/token：通过 char_to_word() 和 char_to_token() 方法来实现：\n",
    "chars = 'My name is Sylvain'\n",
    "print('characters of \"{}\" ars: {}'.format(chars, list(chars)))\n",
    "print('corresponding word index: ')\n",
    "for i, c in enumerate(chars):\n",
    "    print('\"{}\": {} '.format(c, encoding.char_to_word(i)), end=\"\")\n",
    "print(\"\\nword_ids:\",encoding.word_ids())    \n",
    "print('\\ncorresponding token index: ')\n",
    "for i, c in enumerate(chars):\n",
    "    print('\"{}\": {} '.format(c, encoding.char_to_token(i)), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 序列标注任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline 的输出\n",
    "前面我们讲过，NER pipeline 模型实际上封装了三个过程：\n",
    "\n",
    "对文本进行编码；\n",
    "将输入送入模型；\n",
    "对模型输出进行后处理。\n",
    "前两个步骤在所有 pipeline 模型中都是一样的，只有第三个步骤——对模型输出进行后处理，则是根据任务类型而不同。token 分类 pipeline 模型在默认情况下会加载 dbmdz/bert-large-cased-finetuned-conll03-english NER 模型，我们直接打印出它的输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.99938285, 'index': 4, 'word': 'S', 'start': 11, 'end': 12}, {'entity': 'I-PER', 'score': 0.99815494, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14}, {'entity': 'I-PER', 'score': 0.99590707, 'index': 6, 'word': '##va', 'start': 14, 'end': 16}, {'entity': 'I-PER', 'score': 0.99923277, 'index': 7, 'word': '##in', 'start': 16, 'end': 18}, {'entity': 'I-ORG', 'score': 0.9738931, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35}, {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40}, {'entity': 'I-ORG', 'score': 0.9887976, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45}, {'entity': 'I-LOC', 'score': 0.9932106, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "results = token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.9796019, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.9932106, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# 实际上，NER pipeline 模型提供了多种组合 token 形成实体的策略，可以通过 aggregation_strategy 参数进行设置：\n",
    "\n",
    "# simple：默认策略，以实体对应所有 token 的平均分数作为得分，例如“Sylvain”的分数就是“S”、“##yl”、“##va”和“##in”四个 token 分数的平均；\n",
    "# first：将第一个 token 的分数作为实体的分数，例如“Sylvain”的分数就是 token “S”的分数；\n",
    "# max：将 token 中最大的分数作为整个实体的分数；\n",
    "# average：对应词语（注意不是 token）的平均分数作为整个实体的分数，例如“Hugging Face”就是“Hugging”（0.975）和 “Face”（0.98879）的平均值 0.9819，而 simple 策略得分为 0.9796。\n",
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
    "results = token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造模型输出\n",
    "通过 AutoModelForTokenClassification 类来构造一个 token 分类模型，并且手工地对模型的输出进行后处理，获得与 pipeline 模型相同的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: {'input_ids': tensor([[  101,  1422,  1271,  1110,   156,  7777,  2497,  1394,  1105,   146,\n",
      "          1250,  1120, 20164, 10932, 10289,  1107,  6010,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 19])\n",
      "outputs: TokenClassifierOutput(loss=None, logits=tensor([[[ 8.7508, -2.2626, -1.5300, -2.2889, -0.6513, -2.0016, -0.0112,\n",
      "          -2.0860,  0.3335],\n",
      "         [ 8.4973, -2.3986, -1.3582, -2.7887,  0.7575, -1.8873,  0.4344,\n",
      "          -1.9900, -0.3397],\n",
      "         [ 9.4719, -2.2261, -0.9849, -2.6116,  0.1219, -2.0627, -0.1259,\n",
      "          -1.8758, -0.0609],\n",
      "         [ 9.8670, -2.2175, -1.3125, -2.4866, -0.2550, -1.8536,  0.0856,\n",
      "          -1.7520, -0.6437],\n",
      "         [-0.2011, -2.1873, -1.5316, -2.7110,  8.4025, -2.4168, -0.6980,\n",
      "          -3.0337, -0.0997],\n",
      "         [ 0.1065, -2.0520, -1.4787, -2.8139,  7.4525, -2.8399, -0.0626,\n",
      "          -3.3666, -0.4683],\n",
      "         [ 0.5985, -2.2538, -1.1926, -3.0111,  7.0070, -2.8675,  0.3492,\n",
      "          -3.3129, -0.2878],\n",
      "         [-0.0584, -2.2660, -1.4335, -3.1940,  8.3225, -2.6212, -0.0348,\n",
      "          -2.9780, -0.2957],\n",
      "         [ 9.6889, -2.4281, -1.5653, -2.5225, -0.9693, -1.5668,  0.4285,\n",
      "          -1.9413, -0.6774],\n",
      "         [ 9.0116, -2.1216, -1.4140, -2.6964,  0.2728, -1.7851,  0.3635,\n",
      "          -1.8407, -0.5922],\n",
      "         [ 9.5258, -2.2616, -1.4557, -2.9603, -0.1311, -1.7799,  0.9169,\n",
      "          -2.2549, -0.9692],\n",
      "         [ 9.1087, -2.2834, -1.3437, -2.8742, -0.2521, -1.5712,  1.1501,\n",
      "          -2.0786, -0.8658],\n",
      "         [ 2.5185, -3.1537, -1.6923, -3.4240,  1.4335, -1.8089,  6.5008,\n",
      "          -3.0264, -0.2619],\n",
      "         [ 1.7707, -2.4992, -0.1088, -3.2825,  0.4034, -1.4262,  5.9701,\n",
      "          -2.6502, -0.1259],\n",
      "         [ 0.6466, -2.9276, -0.1020, -3.0776,  0.7036, -1.2746,  6.3889,\n",
      "          -2.7266,  0.3822],\n",
      "         [ 9.2571, -2.6779, -1.2145, -2.7276, -0.9370, -1.5445,  1.1025,\n",
      "          -1.8477, -0.3661],\n",
      "         [-0.2206, -2.5108, -1.2976, -2.9758, -0.5795, -2.2071,  1.8236,\n",
      "          -1.6484,  7.0975],\n",
      "         [ 8.7507, -2.2626, -1.5300, -2.2890, -0.6513, -2.0016, -0.0112,\n",
      "          -2.0860,  0.3335],\n",
      "         [ 8.7508, -2.2626, -1.5300, -2.2889, -0.6513, -2.0016, -0.0112,\n",
      "          -2.0860,  0.3335]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "torch.Size([1, 19, 9])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(\"inputs:\",inputs)\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "print(\"outputs:\",outputs)\n",
    "#模型的输入是一个长度为 \n",
    "#  19的 token 序列，输出尺寸为 1*19*9\n",
    "# ，即模型对每个 token 都会输出一个包含 9 个 logits 值的向量（9 分类）\n",
    "print(outputs.logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 8.7508, -2.2626, -1.5300, -2.2889, -0.6513, -2.0016, -0.0112,\n",
      "          -2.0860,  0.3335],\n",
      "         [ 8.4973, -2.3986, -1.3582, -2.7887,  0.7575, -1.8873,  0.4344,\n",
      "          -1.9900, -0.3397],\n",
      "         [ 9.4719, -2.2261, -0.9849, -2.6116,  0.1219, -2.0627, -0.1259,\n",
      "          -1.8758, -0.0609],\n",
      "         [ 9.8670, -2.2175, -1.3125, -2.4866, -0.2550, -1.8536,  0.0856,\n",
      "          -1.7520, -0.6437],\n",
      "         [-0.2011, -2.1873, -1.5316, -2.7110,  8.4025, -2.4168, -0.6980,\n",
      "          -3.0337, -0.0997],\n",
      "         [ 0.1065, -2.0520, -1.4787, -2.8139,  7.4525, -2.8399, -0.0626,\n",
      "          -3.3666, -0.4683],\n",
      "         [ 0.5985, -2.2538, -1.1926, -3.0111,  7.0070, -2.8675,  0.3492,\n",
      "          -3.3129, -0.2878],\n",
      "         [-0.0584, -2.2660, -1.4335, -3.1940,  8.3225, -2.6212, -0.0348,\n",
      "          -2.9780, -0.2957],\n",
      "         [ 9.6889, -2.4281, -1.5653, -2.5225, -0.9693, -1.5668,  0.4285,\n",
      "          -1.9413, -0.6774],\n",
      "         [ 9.0116, -2.1216, -1.4140, -2.6964,  0.2728, -1.7851,  0.3635,\n",
      "          -1.8407, -0.5922],\n",
      "         [ 9.5258, -2.2616, -1.4557, -2.9603, -0.1311, -1.7799,  0.9169,\n",
      "          -2.2549, -0.9692],\n",
      "         [ 9.1087, -2.2834, -1.3437, -2.8742, -0.2521, -1.5712,  1.1501,\n",
      "          -2.0786, -0.8658],\n",
      "         [ 2.5185, -3.1537, -1.6923, -3.4240,  1.4335, -1.8089,  6.5008,\n",
      "          -3.0264, -0.2619],\n",
      "         [ 1.7707, -2.4992, -0.1088, -3.2825,  0.4034, -1.4262,  5.9701,\n",
      "          -2.6502, -0.1259],\n",
      "         [ 0.6466, -2.9276, -0.1020, -3.0776,  0.7036, -1.2746,  6.3889,\n",
      "          -2.7266,  0.3822],\n",
      "         [ 9.2571, -2.6779, -1.2145, -2.7276, -0.9370, -1.5445,  1.1025,\n",
      "          -1.8477, -0.3661],\n",
      "         [-0.2206, -2.5108, -1.2976, -2.9758, -0.5795, -2.2071,  1.8236,\n",
      "          -1.6484,  7.0975],\n",
      "         [ 8.7507, -2.2626, -1.5300, -2.2890, -0.6513, -2.0016, -0.0112,\n",
      "          -2.0860,  0.3335],\n",
      "         [ 8.7508, -2.2626, -1.5300, -2.2889, -0.6513, -2.0016, -0.0112,\n",
      "          -2.0860,  0.3335]]], grad_fn=<ViewBackward0>)\n",
      "[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]\n",
      "[{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S'}, {'entity': 'I-PER', 'score': 0.9981549382209778, 'word': '##yl'}, {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va'}, {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in'}, {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu'}, {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging'}, {'entity': 'I-ORG', 'score': 0.9887976050376892, 'word': 'Face'}, {'entity': 'I-LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits)\n",
    "import torch\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "# print(probabilities)\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "#{0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC'}\n",
    "print(predictions)\n",
    "\n",
    "results = []\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        results.append(\n",
    "            {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]}\n",
    "        )\n",
    "\n",
    "print(results)\n",
    "# 0.99938285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测类别索引: tensor([1, 1, 2, 3])\n",
      "预测类别标签: ['B', 'B', 'C', 'O']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设张量形状为 (sequence_length, num_classes)，这里sequence_length=4，num_classes=4\n",
    "logits = torch.tensor([\n",
    "    [0.1, 0.9, 0.05, 0.01],  # Token 1, 类别B最有可能\n",
    "    [0.3, 0.5, 0.2, 0.01],  # Token 2, 类别A或B都有可能\n",
    "    [0.05, 0.05, 0.85, 0.05],  # Token 3, 类别C最有可能\n",
    "    [0.01, 0.01, 0.01, 0.97]  # Token 4, 默认类别最有可能\n",
    "])\n",
    "# dim=-1 表示在最后一个维度（类别维度）上操作\n",
    "predicted_classes_indices = logits.argmax(dim=-1)\n",
    "print(\"预测类别索引:\", predicted_classes_indices)\n",
    "# 转换为类别列表\n",
    "# 假设我们知道类别ID与类别标签的映射如下：\n",
    "\n",
    "# 索引0 -> \"A\"\n",
    "# 索引1 -> \"B\"\n",
    "# 索引2 -> \"C\"\n",
    "# 索引3 -> \"O\"（默认类别，非实体）\n",
    "\n",
    "# 假设的类别ID到标签的映射\n",
    "class_labels = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"O\"}\n",
    "\n",
    "# 将索引转换为类别标签\n",
    "predicted_class_labels = [class_labels[i.item()] for i in predicted_classes_indices]\n",
    "print(\"预测类别标签:\", predicted_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32), (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "# print(inputs_with_offsets)\n",
    "offset_mapping = inputs_with_offsets[\"offset_mapping\"]\n",
    "print(offset_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9994322657585144, 1.6470299669890665e-05, 3.426703187869862e-05, 1.6042311472119763e-05, 8.250691462308168e-05, 2.1382315026130527e-05, 0.00015649104898329824, 1.9652115952339955e-05, 0.0002208924270235002], [0.9989632368087769, 1.851575871114619e-05, 5.240453174337745e-05, 1.2534721463453025e-05, 0.0004347366630099714, 3.087430013692938e-05, 0.0003146875824313611, 2.7860676709678955e-05, 0.00014510867185890675], [0.9997084736824036, 8.308127689815592e-06, 2.8745616873493418e-05, 5.650359071296407e-06, 8.694857388036326e-05, 9.783477253222372e-06, 6.786145968362689e-05, 1.1793993508035783e-05, 7.241901766974479e-05], [0.9998351335525513, 5.6455264711985365e-06, 1.3955141184851527e-05, 4.3133736653544474e-06, 4.0176873881137e-05, 8.123063707898837e-06, 5.648486330755986e-05, 8.991626600618474e-06, 2.7239089831709862e-05], [0.0001833340502344072, 2.515659434720874e-05, 4.8462032282259315e-05, 1.4900553651386872e-05, 0.9993828535079956, 1.999772030103486e-05, 0.00011153621017001569, 1.0790749911393505e-05, 0.00020288894302211702], [0.0006440271390601993, 7.437887688865885e-05, 0.00013196606596466154, 3.471960371825844e-05, 0.9981549382209778, 3.3829655876616016e-05, 0.0005438175867311656, 1.9978178897872567e-05, 0.000362446007784456], [0.0016408422961831093, 9.469492943026125e-05, 0.00027364378911443055, 4.440645716385916e-05, 0.995907187461853, 5.126226460561156e-05, 0.0012787950690835714, 3.2835516321938485e-05, 0.0006763283163309097], [0.00022901866759639233, 2.5183346224366687e-05, 5.789952410850674e-05, 9.95701066131005e-06, 0.9992327690124512, 1.7655107512837276e-05, 0.00023448391584679484, 1.235660420206841e-05, 0.000180650400579907], [0.999804675579071, 5.465036792884348e-06, 1.295000492973486e-05, 4.9724390009942e-06, 2.3503229385823943e-05, 1.2930728189530782e-05, 9.509934898233041e-05, 8.891771358321421e-06, 3.146939707221463e-05], [0.9995046854019165, 1.461188821849646e-05, 2.964693703688681e-05, 8.223420991271269e-06, 0.0001601653202669695, 2.0456824131542817e-05, 0.00017537134408485144, 1.934913598233834e-05, 6.743986159563065e-05], [0.9996775388717651, 7.5969492172589526e-06, 1.7006908819894306e-05, 3.7776069348183228e-06, 6.396068056346849e-05, 1.2297842658881564e-05, 0.0001824141072575003, 7.648388418601826e-06, 2.7664606022881344e-05], [0.999434769153595, 1.1278317288088147e-05, 2.8862716135336086e-05, 6.246597422432387e-06, 8.598279964644462e-05, 2.2989384888205677e-05, 0.00034945228253491223, 1.3841326108376961e-05, 4.654669464798644e-05], [0.018156280741095543, 6.245910481084138e-05, 0.0002693218702916056, 4.766615529661067e-05, 0.006134603172540665, 0.0002396739728283137, 0.9738931059837341, 7.093795284163207e-05, 0.001125913579016924], [0.014645983465015888, 0.0002047977759502828, 0.0022360258735716343, 9.357065573567525e-05, 0.003731628181412816, 0.0005988662014715374, 0.9761149883270264, 0.00017609840142540634, 0.002198093105107546], [0.003171567339450121, 8.892157347872853e-05, 0.0015001432038843632, 7.652999192941934e-05, 0.0033575601410120726, 0.0004643817665055394, 0.9887976050376892, 0.0001087131749955006, 0.002434592926874757], [0.9995326995849609, 6.553959792654496e-06, 2.8316144380369224e-05, 6.235935870790854e-06, 3.737203587661497e-05, 2.035713077930268e-05, 0.00028727506287395954, 1.5032625924504828e-05, 6.614292215090245e-05], [0.0006589226541109383, 6.67124186293222e-05, 0.00022443967463914305, 4.190392792224884e-05, 0.0004602030967362225, 9.038783173309639e-05, 0.005088842008262873, 0.0001580320531502366, 0.9932106137275696], [0.9994321465492249, 1.6470674381707795e-05, 3.426761759328656e-05, 1.604246426722966e-05, 8.250903192674741e-05, 2.1382353224908002e-05, 0.00015649342094548047, 1.9652299670269713e-05, 0.00022089408594183624], [0.9994322657585144, 1.6470299669890665e-05, 3.426703187869862e-05, 1.6042327843024395e-05, 8.250683458754793e-05, 2.1382315026130527e-05, 0.00015649090346414596, 1.965213414223399e-05, 0.0002208926307503134]]\n",
      "[{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S', 'start': 11, 'end': 12}, {'entity': 'I-PER', 'score': 0.9981549382209778, 'word': '##yl', 'start': 12, 'end': 14}, {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va', 'start': 14, 'end': 16}, {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in', 'start': 16, 'end': 18}, {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu', 'start': 33, 'end': 35}, {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging', 'start': 35, 'end': 40}, {'entity': 'I-ORG', 'score': 0.9887976050376892, 'word': 'Face', 'start': 41, 'end': 45}, {'entity': 'I-LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "import torch\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "print(probabilities)\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        start, end = offsets[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity\": label,\n",
    "                \"score\": probabilities[idx][pred],\n",
    "                \"word\": tokens[idx],\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 组合实体\n",
    "我们以前面介绍的 simple 合并策略为例，将连续的标签为“I-XXX”的多个 token 进行合并（或者以“B-XXX”开头，后面接多个“I-XXX”的 token 序列），直到遇到\n",
    "\n",
    "“O”：表示该 token 为非实体；\n",
    "“B-XXX”或“I-YYY”或“B-YYY”：表示出现了新的实体。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9981694370508194, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.9796018997828165, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "import torch\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        label = label[2:] # Remove the B- or I-\n",
    "        start, end = offsets[idx]\n",
    "        all_scores = [probabilities[idx][pred]]\n",
    "        # Grab all the tokens labeled with I-label\n",
    "        while (\n",
    "            idx + 1 < len(predictions)\n",
    "            and model.config.id2label[predictions[idx + 1]] == f\"I-{label}\"\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx + 1][predictions[idx + 1]])\n",
    "            _, end = offsets[idx + 1]\n",
    "            idx += 1\n",
    "\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 抽取式问答任务\n",
    "除了序列标注以外，抽取式问答是另一个需要使用到分词器高级功能的任务。与 NER 任务类似，自动问答需要根据问题从原文中标记（抽取）出答案片段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9741135835647583, 'start': 76, 'end': 104, 'answer': 'Jax, PyTorch, and TensorFlow'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back Transformers?\"\n",
    "results = question_answerer(question=question, context=context)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 65])\n",
      "tensor([[-3.9582, -5.9036, -3.9443, -6.2181, -6.4083, -7.1622, -6.0465, -5.1919,\n",
      "         -4.0218,  1.1040, -3.9652, -1.5413, -2.2242,  3.1515,  6.2945, -0.4716,\n",
      "         -1.4831, -0.5067, -4.3221, -1.6551,  2.5779, 10.9044, -0.6544,  2.6956,\n",
      "         -1.1208, -1.6860, -3.7357, -1.6676, -1.5421, -1.8649,  2.0896, -1.2079,\n",
      "         -0.7890,  0.0215, -1.3682, -3.5892, -4.3107, -3.8289, -7.1438, -5.9742,\n",
      "         -3.7412, -5.6779, -4.2294, -4.4258, -2.2509, -6.1912, -7.2860, -3.6947,\n",
      "         -6.6102, -3.8975, -3.4443, -2.6780, -7.3615, -4.1177, -6.7804, -4.3929,\n",
      "         -6.6827, -7.4341, -5.9426, -6.6557, -8.2156, -6.9574, -6.2020, -6.1046,\n",
      "         -4.0218]], grad_fn=<CloneBackward0>)\n",
      "tensor([[-1.7854, -6.2361, -6.2518, -5.3445, -5.2671, -8.1038, -5.0321, -5.9211,\n",
      "         -3.2730, -0.7021, -6.1406, -4.3293, -5.8735, -4.2517,  4.9747, -3.4800,\n",
      "          0.0339, -3.4037, -1.4726,  2.4518, -0.8068,  2.2278,  0.7126, -0.5041,\n",
      "          0.2587, -0.3865, -0.6514,  4.5269,  2.0128,  0.7227,  2.0128,  2.6679,\n",
      "          1.7589, 11.4564,  7.0150, -4.5284, -6.1090, -5.9652, -6.3215, -4.4237,\n",
      "         -2.4921, -3.5017,  2.8051,  0.4060, -5.7715, -6.6872, -7.4680, -4.7417,\n",
      "         -8.0210, -4.9343, -4.8170, -1.4989, -7.1752, -1.5023, -6.4373, -5.4378,\n",
      "         -3.4248, -7.3257, -7.2739, -3.7352, -6.9190, -6.4329, -1.2230, -0.7769,\n",
      "         -3.2730]], grad_fn=<CloneBackward0>)\n",
      "torch.Size([1, 65]) torch.Size([1, 65])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "context = \"\"\"\n",
    "Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back Transformers?\"\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits)\n",
    "print(end_logits)\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Hugging Face的Transformers库中，当你使用像BERT、DistilBERT这样的模型时，inputs.sequence_ids()方法主要用于识别输入序列中不同部分的标记。\n",
    "这个方法对于区分原始文本、问题文本、以及可能的填充或特殊标记（如[CLS]、[SEP]）非常重要，尤其是在处理诸如问答、文本分类等任务的输入数据时。\n",
    "\n",
    "具体来说，sequence_ids()返回一个列表，列表中的每个元素对应输入序列中的一个标记（token），元素值用来指示该标记属于输入中的哪个部分。常见的值包括：\n",
    "\n",
    "0：通常表示序列的开始或分类任务中的[CLS]标记。\n",
    "1：一般代表第一个序列的实际文本内容，比如在问答任务中就是上下文部分。\n",
    "2：在某些情况下，如果存在第二个序列（如问题部分），那么它代表第二个序列的实际文本内容。\n",
    "例如，在一个典型的问答任务中，输入序列会以[CLS]标记开始，接着是上下文文本，然后是一个[SEP]标记，\n",
    "之后是问题文本，最后以另一个[SEP]标记结束。sequence_ids()可以帮助我们区分这些不同的部分，这对于后续处理，比如应用不同的mask策略或分析模型行为，非常有用。\n",
    "\n",
    "在上述代码片段中，通过sequence_ids = inputs.sequence_ids()获得的sequence_ids列表，后续被用来创建一个掩码（mask），该掩码能够确保在计算答案起始和结束位置的概率时，只考虑上下文和问题的实际文本部分，而忽略特殊标记和填充内容，从而提高了预测答案的准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "context = \"\"\"\n",
    "Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back Transformers?\"\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "# 处理logits：从模型输出中提取出start_logits和end_logits，它们分别代表每个词位作为答案起始和结束位置的可能性。\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "# print(start_logits)\n",
    "# print(end_logits)\n",
    "import torch\n",
    "# 创建并应用掩码：为了确保模型预测的答案仅限于实际的文本内容而非其他特殊标记或填充内容，代码通过\n",
    "# inputs.sequence_ids()获取序列ID，并据此创建了一个布尔掩码mask。接着调整掩码，确保不考虑序列中的特殊标记，\n",
    "# 特别是将[CLS]标记对应的掩码设为False，\n",
    "# 因为它是句子的开始，通常不包含答案信息。最后，将此掩码应用于logits，将非答案区域的分数设为一个很低的值（-10000），以排除其被选为答案的可能性。\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "# print(\"sequence_ids:\",sequence_ids)\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# print(\"开始或分类任务中的[CLS]标记的掩码:\",mask)\n",
    "mask[0] = False # Unmask the [CLS] token\n",
    "# print(mask)\n",
    "mask = torch.tensor(mask)[None]\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000\n",
    "# print(start_logits)\n",
    "# print(end_logits)\n",
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]\n",
    "\n",
    "# print(start_probabilities)\n",
    "# print(end_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = start_probabilities[:, None] * end_probabilities[None, :]\n",
    "scores = torch.triu(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
